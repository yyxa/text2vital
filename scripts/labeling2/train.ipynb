{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.json\n",
      "347.json\n",
      "227.json\n",
      "153.json\n",
      "136.json\n",
      "196.json\n",
      "50.json\n",
      "9.json\n",
      "395.json\n",
      "203.json\n",
      "131.json\n",
      "231.json\n",
      "332.json\n",
      "189.json\n",
      "157.json\n",
      "34.json\n",
      "226.json\n",
      "324.json\n",
      "63.json\n",
      "128.json\n",
      "79.json\n",
      "323.json\n",
      "48.json\n",
      "105.json\n",
      "218.json\n",
      "172.json\n",
      "186.json\n",
      "182.json\n",
      "239.json\n",
      "127.json\n",
      "165.json\n",
      "288.json\n",
      "268.json\n",
      "134.json\n",
      "49.json\n",
      "382.json\n",
      "140.json\n",
      "381.json\n",
      "144.json\n",
      "291.json\n",
      "286.json\n",
      "117.json\n",
      "237.json\n",
      "219.json\n",
      "83.json\n",
      "57.json\n",
      "200.json\n",
      "246.json\n",
      "389.json\n",
      "36.json\n",
      "120.json\n",
      "283.json\n",
      "86.json\n",
      "40.json\n",
      "129.json\n",
      "245.json\n",
      "75.json\n",
      "185.json\n",
      "356.json\n",
      "107.json\n",
      "341.json\n",
      "142.json\n",
      "215.json\n",
      "370.json\n",
      "352.json\n",
      "279.json\n",
      "33.json\n",
      "374.json\n",
      "336.json\n",
      "232.json\n",
      "10.json\n",
      "139.json\n",
      "339.json\n",
      "65.json\n",
      "74.json\n",
      "187.json\n",
      "158.json\n",
      "72.json\n",
      "309.json\n",
      "307.json\n",
      "141.json\n",
      "176.json\n",
      "292.json\n",
      "114.json\n",
      "191.json\n",
      "11.json\n",
      "257.json\n",
      "375.json\n",
      "380.json\n",
      "360.json\n",
      "320.json\n",
      "122.json\n",
      "261.json\n",
      "143.json\n",
      "173.json\n",
      "73.json\n",
      "206.json\n",
      "138.json\n",
      "38.json\n",
      "21.json\n",
      "62.json\n",
      "328.json\n",
      "179.json\n",
      "61.json\n",
      "260.json\n",
      "214.json\n",
      "222.json\n",
      "305.json\n",
      "228.json\n",
      "255.json\n",
      "202.json\n",
      "362.json\n",
      "262.json\n",
      "8.json\n",
      "265.json\n",
      "188.json\n",
      "282.json\n",
      "184.json\n",
      "365.json\n",
      "270.json\n",
      "6.json\n",
      "164.json\n",
      "149.json\n",
      "170.json\n",
      "376.json\n",
      "273.json\n",
      "383.json\n",
      "335.json\n",
      "201.json\n",
      "155.json\n",
      "302.json\n",
      "314.json\n",
      "5.json\n",
      "96.json\n",
      "306.json\n",
      "348.json\n",
      "264.json\n",
      "390.json\n",
      "81.json\n",
      "220.json\n",
      "399.json\n",
      "77.json\n",
      "317.json\n",
      "301.json\n",
      "247.json\n",
      "112.json\n",
      "180.json\n",
      "313.json\n",
      "166.json\n",
      "177.json\n",
      "318.json\n",
      "259.json\n",
      "4.json\n",
      "208.json\n",
      "369.json\n",
      "76.json\n",
      "60.json\n",
      "311.json\n",
      "146.json\n",
      "303.json\n",
      "67.json\n",
      "344.json\n",
      "13.json\n",
      "327.json\n",
      "178.json\n",
      "297.json\n",
      "91.json\n",
      "195.json\n",
      "192.json\n",
      "281.json\n",
      "20.json\n",
      "12.json\n",
      "115.json\n",
      "361.json\n",
      "55.json\n",
      "233.json\n",
      "393.json\n",
      "248.json\n",
      "280.json\n",
      "357.json\n",
      "161.json\n",
      "242.json\n",
      "209.json\n",
      "198.json\n",
      "331.json\n",
      "3.json\n",
      "110.json\n",
      "363.json\n",
      "266.json\n",
      "145.json\n",
      "343.json\n",
      "263.json\n",
      "26.json\n",
      "132.json\n",
      "163.json\n",
      "353.json\n",
      "51.json\n",
      "175.json\n",
      "275.json\n",
      "22.json\n",
      "193.json\n",
      "250.json\n",
      "234.json\n",
      "249.json\n",
      "54.json\n",
      "284.json\n",
      "337.json\n",
      "321.json\n",
      "287.json\n",
      "285.json\n",
      "89.json\n",
      "400.json\n",
      "224.json\n",
      "39.json\n",
      "190.json\n",
      "316.json\n",
      "154.json\n",
      "102.json\n",
      "64.json\n",
      "68.json\n",
      "211.json\n",
      "24.json\n",
      "256.json\n",
      "167.json\n",
      "169.json\n",
      "240.json\n",
      "104.json\n",
      "330.json\n",
      "159.json\n",
      "151.json\n",
      "258.json\n",
      "244.json\n",
      "78.json\n",
      "355.json\n",
      "25.json\n",
      "35.json\n",
      "92.json\n",
      "152.json\n",
      "236.json\n",
      "31.json\n",
      "329.json\n",
      "46.json\n",
      "82.json\n",
      "88.json\n",
      "23.json\n",
      "1.json\n",
      "160.json\n",
      "251.json\n",
      "207.json\n",
      "310.json\n",
      "345.json\n",
      "359.json\n",
      "27.json\n",
      "204.json\n",
      "123.json\n",
      "366.json\n",
      "377.json\n",
      "168.json\n",
      "69.json\n",
      "7.json\n",
      "293.json\n",
      "267.json\n",
      "135.json\n",
      "59.json\n",
      "290.json\n",
      "29.json\n",
      "315.json\n",
      "95.json\n",
      "308.json\n",
      "221.json\n",
      "271.json\n",
      "289.json\n",
      "235.json\n",
      "322.json\n",
      "43.json\n",
      "385.json\n",
      "213.json\n",
      "325.json\n",
      "126.json\n",
      "183.json\n",
      "125.json\n",
      "346.json\n",
      "379.json\n",
      "42.json\n",
      "28.json\n",
      "384.json\n",
      "71.json\n",
      "37.json\n",
      "269.json\n",
      "44.json\n",
      "217.json\n",
      "272.json\n",
      "17.json\n",
      "53.json\n",
      "80.json\n",
      "277.json\n",
      "210.json\n",
      "364.json\n",
      "342.json\n",
      "56.json\n",
      "299.json\n",
      "181.json\n",
      "19.json\n",
      "100.json\n",
      "229.json\n",
      "70.json\n",
      "119.json\n",
      "386.json\n",
      "147.json\n",
      "194.json\n",
      "340.json\n",
      "312.json\n",
      "367.json\n",
      "106.json\n",
      "392.json\n",
      "47.json\n",
      "276.json\n",
      "294.json\n",
      "103.json\n",
      "388.json\n",
      "300.json\n",
      "304.json\n",
      "350.json\n",
      "394.json\n",
      "85.json\n",
      "58.json\n",
      "30.json\n",
      "2.json\n",
      "253.json\n",
      "84.json\n",
      "171.json\n",
      "338.json\n",
      "133.json\n",
      "124.json\n",
      "398.json\n",
      "296.json\n",
      "197.json\n",
      "18.json\n",
      "252.json\n",
      "351.json\n",
      "45.json\n",
      "118.json\n",
      "16.json\n",
      "230.json\n",
      "66.json\n",
      "358.json\n",
      "162.json\n",
      "368.json\n",
      "156.json\n",
      "41.json\n",
      "137.json\n",
      "223.json\n",
      "354.json\n",
      "116.json\n",
      "396.json\n",
      "113.json\n",
      "90.json\n",
      "243.json\n",
      "212.json\n",
      "326.json\n",
      "108.json\n",
      "216.json\n",
      "111.json\n",
      "14.json\n",
      "98.json\n",
      "93.json\n",
      "349.json\n",
      "97.json\n",
      "278.json\n",
      "199.json\n",
      "121.json\n",
      "205.json\n",
      "225.json\n",
      "274.json\n",
      "174.json\n",
      "298.json\n",
      "94.json\n",
      "371.json\n",
      "378.json\n",
      "99.json\n",
      "87.json\n",
      "334.json\n",
      "52.json\n",
      "373.json\n",
      "238.json\n",
      "130.json\n",
      "109.json\n",
      "254.json\n",
      "391.json\n",
      "295.json\n",
      "32.json\n",
      "333.json\n",
      "150.json\n",
      "397.json\n",
      "387.json\n",
      "319.json\n",
      "101.json\n",
      "241.json\n",
      "372.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def preprocess_data(input_dir, output_file):\n",
    "    data = []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(input_dir, file_name), 'r') as f:\n",
    "                content = json.load(f)\n",
    "                entry = {\n",
    "                    \"input\": f\"{content['genre']} | {content['type']} | {content['description']}\",\n",
    "                    \"output\": json.dumps(content[\"preset\"][\"settings\"])\n",
    "                }\n",
    "                data.append(entry)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "input_directory = \"labeled_data_music\"\n",
    "output_data_file = \"training_data.json\"\n",
    "preprocess_data(input_directory, output_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.json\n",
      "347.json\n",
      "227.json\n",
      "153.json\n",
      "136.json\n",
      "196.json\n",
      "50.json\n",
      "9.json\n",
      "395.json\n",
      "203.json\n",
      "131.json\n",
      "231.json\n",
      "332.json\n",
      "189.json\n",
      "157.json\n",
      "34.json\n",
      "226.json\n",
      "324.json\n",
      "63.json\n",
      "128.json\n",
      "79.json\n",
      "323.json\n",
      "48.json\n",
      "105.json\n",
      "218.json\n",
      "172.json\n",
      "186.json\n",
      "182.json\n",
      "239.json\n",
      "127.json\n",
      "165.json\n",
      "288.json\n",
      "268.json\n",
      "134.json\n",
      "49.json\n",
      "382.json\n",
      "140.json\n",
      "381.json\n",
      "144.json\n",
      "291.json\n",
      "286.json\n",
      "117.json\n",
      "237.json\n",
      "219.json\n",
      "83.json\n",
      "57.json\n",
      "200.json\n",
      "246.json\n",
      "389.json\n",
      "36.json\n",
      "120.json\n",
      "283.json\n",
      "86.json\n",
      "40.json\n",
      "129.json\n",
      "245.json\n",
      "75.json\n",
      "185.json\n",
      "356.json\n",
      "107.json\n",
      "341.json\n",
      "142.json\n",
      "215.json\n",
      "370.json\n",
      "352.json\n",
      "279.json\n",
      "33.json\n",
      "374.json\n",
      "336.json\n",
      "232.json\n",
      "10.json\n",
      "139.json\n",
      "339.json\n",
      "65.json\n",
      "74.json\n",
      "187.json\n",
      "158.json\n",
      "72.json\n",
      "309.json\n",
      "307.json\n",
      "141.json\n",
      "176.json\n",
      "292.json\n",
      "114.json\n",
      "191.json\n",
      "11.json\n",
      "257.json\n",
      "375.json\n",
      "380.json\n",
      "360.json\n",
      "320.json\n",
      "122.json\n",
      "261.json\n",
      "143.json\n",
      "173.json\n",
      "73.json\n",
      "206.json\n",
      "138.json\n",
      "38.json\n",
      "21.json\n",
      "62.json\n",
      "328.json\n",
      "179.json\n",
      "61.json\n",
      "260.json\n",
      "214.json\n",
      "222.json\n",
      "305.json\n",
      "228.json\n",
      "255.json\n",
      "202.json\n",
      "362.json\n",
      "262.json\n",
      "8.json\n",
      "265.json\n",
      "188.json\n",
      "282.json\n",
      "184.json\n",
      "365.json\n",
      "270.json\n",
      "6.json\n",
      "164.json\n",
      "149.json\n",
      "170.json\n",
      "376.json\n",
      "273.json\n",
      "383.json\n",
      "335.json\n",
      "201.json\n",
      "155.json\n",
      "302.json\n",
      "314.json\n",
      "5.json\n",
      "96.json\n",
      "306.json\n",
      "348.json\n",
      "264.json\n",
      "390.json\n",
      "81.json\n",
      "220.json\n",
      "399.json\n",
      "77.json\n",
      "317.json\n",
      "301.json\n",
      "247.json\n",
      "112.json\n",
      "180.json\n",
      "313.json\n",
      "166.json\n",
      "177.json\n",
      "318.json\n",
      "259.json\n",
      "4.json\n",
      "208.json\n",
      "369.json\n",
      "76.json\n",
      "60.json\n",
      "311.json\n",
      "146.json\n",
      "303.json\n",
      "67.json\n",
      "344.json\n",
      "13.json\n",
      "327.json\n",
      "178.json\n",
      "297.json\n",
      "91.json\n",
      "195.json\n",
      "192.json\n",
      "281.json\n",
      "20.json\n",
      "12.json\n",
      "115.json\n",
      "361.json\n",
      "55.json\n",
      "233.json\n",
      "393.json\n",
      "248.json\n",
      "280.json\n",
      "357.json\n",
      "161.json\n",
      "242.json\n",
      "209.json\n",
      "198.json\n",
      "331.json\n",
      "3.json\n",
      "110.json\n",
      "363.json\n",
      "266.json\n",
      "145.json\n",
      "343.json\n",
      "263.json\n",
      "26.json\n",
      "132.json\n",
      "163.json\n",
      "353.json\n",
      "51.json\n",
      "175.json\n",
      "275.json\n",
      "22.json\n",
      "193.json\n",
      "250.json\n",
      "234.json\n",
      "249.json\n",
      "54.json\n",
      "284.json\n",
      "337.json\n",
      "321.json\n",
      "287.json\n",
      "285.json\n",
      "89.json\n",
      "400.json\n",
      "224.json\n",
      "39.json\n",
      "190.json\n",
      "316.json\n",
      "154.json\n",
      "102.json\n",
      "64.json\n",
      "68.json\n",
      "211.json\n",
      "24.json\n",
      "256.json\n",
      "167.json\n",
      "169.json\n",
      "240.json\n",
      "104.json\n",
      "330.json\n",
      "159.json\n",
      "151.json\n",
      "258.json\n",
      "244.json\n",
      "78.json\n",
      "355.json\n",
      "25.json\n",
      "35.json\n",
      "92.json\n",
      "152.json\n",
      "236.json\n",
      "31.json\n",
      "329.json\n",
      "46.json\n",
      "82.json\n",
      "88.json\n",
      "23.json\n",
      "1.json\n",
      "160.json\n",
      "251.json\n",
      "207.json\n",
      "310.json\n",
      "345.json\n",
      "359.json\n",
      "27.json\n",
      "204.json\n",
      "123.json\n",
      "366.json\n",
      "377.json\n",
      "168.json\n",
      "69.json\n",
      "7.json\n",
      "293.json\n",
      "267.json\n",
      "135.json\n",
      "59.json\n",
      "290.json\n",
      "29.json\n",
      "315.json\n",
      "95.json\n",
      "308.json\n",
      "221.json\n",
      "271.json\n",
      "289.json\n",
      "235.json\n",
      "322.json\n",
      "43.json\n",
      "385.json\n",
      "213.json\n",
      "325.json\n",
      "126.json\n",
      "183.json\n",
      "125.json\n",
      "346.json\n",
      "379.json\n",
      "42.json\n",
      "28.json\n",
      "384.json\n",
      "71.json\n",
      "37.json\n",
      "269.json\n",
      "44.json\n",
      "217.json\n",
      "272.json\n",
      "17.json\n",
      "53.json\n",
      "80.json\n",
      "277.json\n",
      "210.json\n",
      "364.json\n",
      "342.json\n",
      "56.json\n",
      "299.json\n",
      "181.json\n",
      "19.json\n",
      "100.json\n",
      "229.json\n",
      "70.json\n",
      "119.json\n",
      "386.json\n",
      "147.json\n",
      "194.json\n",
      "340.json\n",
      "312.json\n",
      "367.json\n",
      "106.json\n",
      "392.json\n",
      "47.json\n",
      "276.json\n",
      "294.json\n",
      "103.json\n",
      "388.json\n",
      "300.json\n",
      "304.json\n",
      "350.json\n",
      "394.json\n",
      "85.json\n",
      "58.json\n",
      "30.json\n",
      "2.json\n",
      "253.json\n",
      "84.json\n",
      "171.json\n",
      "338.json\n",
      "133.json\n",
      "124.json\n",
      "398.json\n",
      "296.json\n",
      "197.json\n",
      "18.json\n",
      "252.json\n",
      "351.json\n",
      "45.json\n",
      "118.json\n",
      "16.json\n",
      "230.json\n",
      "66.json\n",
      "358.json\n",
      "162.json\n",
      "368.json\n",
      "156.json\n",
      "41.json\n",
      "137.json\n",
      "223.json\n",
      "354.json\n",
      "116.json\n",
      "396.json\n",
      "113.json\n",
      "90.json\n",
      "243.json\n",
      "212.json\n",
      "326.json\n",
      "108.json\n",
      "216.json\n",
      "111.json\n",
      "14.json\n",
      "98.json\n",
      "93.json\n",
      "349.json\n",
      "97.json\n",
      "278.json\n",
      "199.json\n",
      "121.json\n",
      "205.json\n",
      "225.json\n",
      "274.json\n",
      "174.json\n",
      "298.json\n",
      "94.json\n",
      "371.json\n",
      "378.json\n",
      "99.json\n",
      "87.json\n",
      "334.json\n",
      "52.json\n",
      "373.json\n",
      "238.json\n",
      "130.json\n",
      "109.json\n",
      "254.json\n",
      "391.json\n",
      "295.json\n",
      "32.json\n",
      "333.json\n",
      "150.json\n",
      "397.json\n",
      "387.json\n",
      "319.json\n",
      "101.json\n",
      "241.json\n",
      "372.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def preprocess_data(input_dir, output_file):\n",
    "    data = []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        print(file_name)\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(input_dir, file_name), 'r') as f:\n",
    "                content = json.load(f)\n",
    "                output_values = [value for key, value in content[\"preset\"][\"settings\"].items() if isinstance(value, (int, float))]\n",
    "                entry = {\n",
    "                    \"input\": f\"{content['genre']} | {content['type']} | {content['description']}\",\n",
    "                    \"output\": json.dumps(output_values)\n",
    "                }\n",
    "                data.append(entry)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "input_directory = \"labeled_data_music\"\n",
    "output_data_file = \"training_data.json\"\n",
    "preprocess_data(input_directory, output_data_file)\n",
    "# custom_warps, lfos, modulations, random_values, sample, wavetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: sentencepiece in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: accelerate in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: evaluate in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (0.4.3)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: dill in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: click in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m41.3 kB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24951 sha256=4f06ca812ebbc6a527880ee2e4986316ec5aeabe9fc83924d84262ec2dfb0b6c\n",
      "  Stored in directory: /home/drama/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: nltk, rouge_score\n",
      "Successfully installed nltk-3.9.1 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece torch accelerate evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:39:04.666022: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 13:39:04.675505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733135944.687001 1033991 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733135944.690602 1033991 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 13:39:04.707125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/drama/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/drama/projects/active/text2vital/scripts/labeling2/wandb/run-20241202_133916-ya8yahhf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yyxa-nust-misis/huggingface/runs/ya8yahhf' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/yyxa-nust-misis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yyxa-nust-misis/huggingface' target=\"_blank\">https://wandb.ai/yyxa-nust-misis/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yyxa-nust-misis/huggingface/runs/ya8yahhf' target=\"_blank\">https://wandb.ai/yyxa-nust-misis/huggingface/runs/ya8yahhf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e760bf6575604b88b31a41485eb60485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2028.5367, 'train_samples_per_second': 0.59, 'train_steps_per_second': 0.074, 'train_loss': 2.5594342041015623, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=2.5594342041015623, metrics={'train_runtime': 2028.5367, 'train_samples_per_second': 0.59, 'train_steps_per_second': 0.074, 'total_flos': 162004136361984.0, 'train_loss': 2.5594342041015623, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SynthPresetDataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_length=512):\n",
    "        with open(data_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_enc = self.tokenizer(item[\"input\"], max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        output_enc = self.tokenizer(item[\"output\"], max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": input_enc.attention_mask.squeeze(),\n",
    "            \"labels\": output_enc.input_ids.squeeze()\n",
    "        }\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "dataset = SynthPresetDataset(\"training_data.json\", tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# 312bcaf95323b58c7785fb87c1bf62211880ca96\n",
    "trainer.train()\n",
    "tokenizer.save_pretrained(\"./results/final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 18:43:50.539671: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 18:43:50.748309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733154230.827506 1107063 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733154230.849591 1107063 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 18:43:51.052573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Output: \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"Like, \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\": \"spread\":\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 9 (char 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuzzy, distorted bass with a heavy kick, ideal for intense moments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreset.vital\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mgenerate_preset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mgenerate_preset\u001b[0;34m(description, model, tokenizer, output_file, max_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_output)\n\u001b[0;32m---> 16\u001b[0m preset_settings \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m vital_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreset_style\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m: preset_settings,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynth_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.5.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 9 (char 8)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"./results/checkpoint-150\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def generate_preset(description, model, tokenizer, output_file, max_length=512):\n",
    "    input_text = f\"{description}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids\n",
    "    outputs = model.generate(input_ids, max_length=max_length)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Decoded Output:\", decoded_output)\n",
    "    preset_settings = json.loads(decoded_output)\n",
    "    \n",
    "    vital_data = {\n",
    "        \"preset_name\": \"TEST1\",\n",
    "        \"preset_style\": \"\",\n",
    "        \"settings\": preset_settings,\n",
    "        \"synth_version\": \"1.5.5\"\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(vital_data, f, indent=4)\n",
    "    print(f\"Preset saved to {output_file}\")\n",
    "\n",
    "description = \"buzzy, distorted bass with a heavy kick, ideal for intense moments\"\n",
    "output_file = \"preset.vital\"\n",
    "generate_preset(description, model, tokenizer, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 14:58:52.882715: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. Load and preprocess the data\n",
    "with open(\"training_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "input_texts = [item[\"input\"] for item in data]\n",
    "output_texts = [item[\"output\"] for item in data]\n",
    "\n",
    "# Add start and end tokens to the output text\n",
    "output_texts = [\"<start> \" + text + \" <end>\" for text in output_texts]\n",
    "\n",
    "# Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# Padding\n",
    "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
    "max_decoder_seq_length = max([len(seq) for seq in output_sequences])\n",
    "\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding=\"post\")\n",
    "decoder_input_data = pad_sequences(output_sequences, maxlen=max_decoder_seq_length, padding=\"post\")\n",
    "\n",
    "# Prepare decoder target data as categorical labels (not one-hot encoded)\n",
    "decoder_target_data = np.zeros((len(output_sequences), max_decoder_seq_length), dtype=\"int32\")\n",
    "for i, seq in enumerate(output_sequences):\n",
    "    for t, word_id in enumerate(seq[1:]):  # Skip the start token\n",
    "        decoder_target_data[i, t] = word_id\n",
    "\n",
    "# 2. Split data into training and validation sets\n",
    "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_target_train, decoder_target_val = train_test_split(\n",
    "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2\n",
    ")\n",
    "\n",
    "# 3. Build the Seq2Seq model\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit(\n",
    "    [encoder_input_train, decoder_input_train],\n",
    "    decoder_target_train,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    ")\n",
    "\n",
    "# 5. Save the model\n",
    "model.save(\"seq2seq_vital.h5\")\n",
    "\n",
    "# 6. Define inference models for testing\n",
    "# Encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Function for generating JSON outputs from input text\n",
    "def generate_json(input_text):\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoder_seq_length, padding=\"post\")\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index[\"<start>\"]\n",
    "\n",
    "    stop_condition = False\n",
    "    generated_sequence = \"\"\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = output_tokenizer.index_word.get(sampled_token_index, \"\")\n",
    "\n",
    "        if sampled_token == \"<end>\" or len(generated_sequence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            generated_sequence += sampled_token\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "# Example usage:\n",
    "print(generate_json(\"Multigenre | Seq | buzzy, distorted bass...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 22:59:35.759471: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 22:59:35.968716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733169576.047933 1252176 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733169576.070278 1252176 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 22:59:36.270896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/drama/projects/active/text2vital/venv/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1252176/3501245206.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myyxa\u001b[0m (\u001b[33myyxa-nust-misis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/drama/projects/active/text2vital/scripts/labeling2/wandb/run-20241202_225944-bua4c4rg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yyxa-nust-misis/huggingface/runs/bua4c4rg' target=\"_blank\">./results2</a></strong> to <a href='https://wandb.ai/yyxa-nust-misis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yyxa-nust-misis/huggingface' target=\"_blank\">https://wandb.ai/yyxa-nust-misis/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yyxa-nust-misis/huggingface/runs/bua4c4rg' target=\"_blank\">https://wandb.ai/yyxa-nust-misis/huggingface/runs/bua4c4rg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264f32a04d514897810344e38ed05c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0054, 'grad_norm': 34.599849700927734, 'learning_rate': 5e-06, 'epoch': 1.25}\n",
      "{'loss': 4.2946, 'grad_norm': 1.7117702960968018, 'learning_rate': 1e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0f7a92afb147a68ea6ce8245d2603e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import json\n",
    "\n",
    "# Загрузка и подготовка данных\n",
    "data_file = \"training_data.json\"\n",
    "dataset = load_dataset(\"json\", data_files=data_file)\n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Токенизация данных\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Преобразование набора данных\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "data_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
    "train_dataset = data_split[\"train\"]\n",
    "val_dataset = data_split[\"test\"]\n",
    "\n",
    "# Определение вычисления метрик\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "# Data Collator для Seq2Seq задач\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Параметры тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    logging_dir=\"./logs2\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение модели и токенизатора\n",
    "trainer.save_model(\"./results2/final_model\")\n",
    "tokenizer.save_pretrained(\"./results2/final_model\")\n",
    "\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"./results/checkpoint-150\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def generate_preset(description, model, tokenizer, output_file, max_length=512):\n",
    "    input_text = f\"{description}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids\n",
    "    outputs = model.generate(input_ids, max_length=max_length)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Decoded Output:\", decoded_output)\n",
    "    preset_settings = json.loads(decoded_output)\n",
    "    \n",
    "    vital_data = {\n",
    "        \"preset_name\": \"TEST1\",\n",
    "        \"preset_style\": \"\",\n",
    "        \"settings\": preset_settings,\n",
    "        \"synth_version\": \"1.5.5\"\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(vital_data, f, indent=4)\n",
    "    print(f\"Preset saved to {output_file}\")\n",
    "\n",
    "description = \"buzzy, distorted bass with a heavy kick, ideal for intense moments\"\n",
    "output_file = \"preset.vital\"\n",
    "generate_preset(description, model, tokenizer, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
