{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name SpectPrompt python=3.9\n",
    "conda activate SpectPrompt\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install transformers datasets librosa einops_exts einops mmcls peft ipdb torchlibrosa\n",
    "pip3 install -U openmim\n",
    "mim install mmcv==1.7.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import nnAudio.Spectrogram\n",
    "\n",
    "class FFT_parameters:\n",
    "    sample_rate = 16000\n",
    "    window_size = 400\n",
    "    n_fft = 400\n",
    "    hop_size = 160\n",
    "    n_mels = 80\n",
    "    f_min = 50\n",
    "    f_max = 8000\n",
    "\n",
    "def wav_to_mel(path: str):\n",
    "    \n",
    "    prms = FFT_parameters()\n",
    "    to_spec = nnAudio.Spectrogram.MelSpectrogram(\n",
    "            sr=prms.sample_rate,\n",
    "            n_fft=prms.n_fft,\n",
    "            win_length=prms.window_size,\n",
    "            hop_length=prms.hop_size,\n",
    "            n_mels=prms.n_mels,\n",
    "            fmin=prms.f_min,\n",
    "            fmax=prms.f_max,\n",
    "            center=True,\n",
    "            power=2,\n",
    "            verbose=False,\n",
    "            )\n",
    "\n",
    "    wav, ori_sr = librosa.load(path, mono=True, sr=prms.sample_rate)\n",
    "    lms = to_spec(torch.tensor(wav))\n",
    "    lms = (lms + torch.finfo().eps).log()\n",
    "    return lms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "post process logMel spectrogram \n",
    "'''\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    This transform does not support PIL Image.\n",
    "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
    "    channels, this transform will normalize each channel of the input\n",
    "    ``torch.*Tensor`` i.e.,\n",
    "    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "\n",
    "    .. note::\n",
    "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
    "\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        # self.inplace = inplace\n",
    "\n",
    "    def __call__(self, imgs) :\n",
    "        return (imgs - self.mean) / self.std\n",
    "\n",
    "\n",
    "class SpecRandomCrop(object):\n",
    "    \"\"\"Spectrogram Random Crop\n",
    "\n",
    "    Args:\n",
    "        target_len (int): Length of target length, \n",
    "            only when input_length > target_length it will work.\n",
    "        p (float, optional): Probability. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_len=960, p=1.):\n",
    "        assert 0 <= p <= 1.0, \\\n",
    "            f'The prob should be in range [0, 1], got {p} instead.'\n",
    "\n",
    "        self.target_len = target_len\n",
    "        self.prob = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() > self.prob:\n",
    "            return img\n",
    "\n",
    "        cnt_len = img.shape[-1]\n",
    "\n",
    "        if cnt_len > self.target_len:\n",
    "            ## TODO\n",
    "\n",
    "            if eval(os.environ.get(\"doing_eval\", 'False')):\n",
    "                crop_start = 0\n",
    "                # print('### evaling crop ####')\n",
    "            else:\n",
    "                crop_start = random.randint(0, cnt_len - self.target_len)\n",
    "\n",
    "            img = img[..., crop_start: crop_start + self.target_len]\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = self.__class__.__name__\n",
    "        repr_str += f'target_length = {self.target_len}, '\n",
    "        repr_str += f'prob = {self.prob}'\n",
    "        return repr_str\n",
    "\n",
    "\n",
    "class SpecPadding(object):\n",
    "    \"\"\"Spectrogram Padding\n",
    "\n",
    "    Args:\n",
    "        target_len (int): Length of target length, tran\n",
    "            if input_length < target_length, it will pad to target len;\n",
    "            if input_length > target_length\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_len=960, padding_method=\"circular\"):\n",
    "\n",
    "        self.target_len = target_len\n",
    "        self.padding_method = padding_method\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # if type(img) is not torch.Tensor:\n",
    "        #     img = torch.tensor(img)\n",
    "        img = self.padding_spec(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = self.__class__.__name__\n",
    "        repr_str += f'target_length = {self.target_len}, '\n",
    "        repr_str += f'padding_method = {self.padding_method}'\n",
    "        return repr_str\n",
    "\n",
    "    def padding_spec(self, img):\n",
    "        \"\"\"\n",
    "        padding 频谱，提供了两种方式\n",
    "        \"\"\"\n",
    "        \n",
    "        cnt_len = img.shape[-1]\n",
    "        if cnt_len > self.target_len:\n",
    "            img = img[..., :self.target_len]\n",
    "            attn_mask = np.ones_like(img) if 'zero' in self.padding_method else None\n",
    "            return img,attn_mask\n",
    "\n",
    "        padding_len = self.target_len - cnt_len\n",
    "\n",
    "        out_tensor = np.zeros((1, img.shape[-2], self.target_len), dtype=img.dtype)\n",
    "        attn_mask = np.zeros_like(out_tensor) if 'zero' in self.padding_method else None\n",
    "        attn_mask_ones = np.ones_like(img)\n",
    "        if self.padding_method == 'circular':\n",
    "            # print(\"cnt_img:\", cnt_img.shape)\n",
    "            # out_tensor[idx] = F.pad(cnt_img.unsqueeze(0), (pad_before,pad_after,0,0), \"circular\")[0]\n",
    "\n",
    "            repeat_times = self.target_len // cnt_len + 1\n",
    "            if repeat_times % 2 == 0:\n",
    "                repeat_times += 1\n",
    "\n",
    "            cnt_img = np.tile(img, (1, 1, repeat_times))\n",
    "\n",
    "            # print(f\"cnt_img_repeat {repeat_times}:\", cnt_img.shape)\n",
    "\n",
    "            side_pad_num = cnt_img.shape[-1] // repeat_times * (repeat_times // 2)\n",
    "            erase_side_pad_num = (cnt_img.shape[-1] - self.target_len + 1) // 2\n",
    "\n",
    "            # print(side_pad_num, erase_side_pad_num)\n",
    "            out_tensor[..., :] = cnt_img[..., erase_side_pad_num:erase_side_pad_num + self.target_len]\n",
    "\n",
    "            # print(\"result:\", out_tensor[idx].shape)\n",
    "\n",
    "            pad_before = side_pad_num - erase_side_pad_num\n",
    "            pad_after = padding_len - pad_before\n",
    "\n",
    "            # print(\"pad:\",pad_before, pad_after)\n",
    "            # pad_lens_list.append((pad_before / float(target_len), pad_after / float(target_len)))\n",
    "\n",
    "        elif self.padding_method == \"random_circular\":\n",
    "\n",
    "            repeat_times = self.target_len // cnt_len + 3\n",
    "\n",
    "            cnt_img = img.repeat((1, 1, repeat_times))\n",
    "\n",
    "            # print(f\"cnt_img_repeat {repeat_times}:\", cnt_img.shape)\n",
    "\n",
    "\n",
    "            # side_pad_num = cnt_img.shape[-1] // repeat_times * (repeat_times // 2)\n",
    "            # erase_side_pad_num = (cnt_img.shape[-1] - self.target_len + 1) // 2\n",
    "            choice_before = random.randint(0, cnt_img.shape[-1] - self.target_len)\n",
    "\n",
    "            # print(side_pad_num, erase_side_pad_num)\n",
    "\n",
    "            out_tensor[..., :] = cnt_img[..., choice_before:choice_before + self.target_len]\n",
    "\n",
    "            # print(\"result:\", out_tensor[idx].shape)\n",
    "\n",
    "            # pad_before = side_pad_num - erase_side_pad_num\n",
    "            # pad_after = padding_len - pad_before\n",
    "\n",
    "            # print(\"pad:\",pad_before, pad_after)\n",
    "            # pad_lens_list.append((pad_before / float(target_len), pad_after / float(target_len)))\n",
    "\n",
    "        elif self.padding_method == 'zero_before':\n",
    "            pad_before = padding_len // 2\n",
    "            pad_after = padding_len - pad_before\n",
    "            out_tensor[..., pad_before:pad_before + cnt_len] = img\n",
    "            attn_mask[..., pad_before:pad_before + cnt_len] = attn_mask_ones\n",
    "            # pad_lens_list.append((pad_before / float(target_len), pad_after / float(target_len)))\n",
    "        elif self.padding_method == \"random_zero\":\n",
    "            pad_before = random.randint(0, padding_len // 2)\n",
    "            pad_after = padding_len - pad_before\n",
    "            out_tensor[..., pad_before:pad_before + cnt_len] = img\n",
    "            attn_mask[..., pad_before:pad_before + cnt_len] = attn_mask_ones\n",
    "\n",
    "        elif self.padding_method == \"zero\":\n",
    "            # audio + zero padding \n",
    "            out_tensor[..., :cnt_len] = img\n",
    "            attn_mask[..., :cnt_len] = attn_mask_ones \n",
    "        else:\n",
    "            assert False, \"padding method should belong to ('circular', 'zero')\"\n",
    "\n",
    "        return out_tensor#, attn_mask\n",
    "\n",
    "class SpecMeanCrop(object):\n",
    "    \"\"\"\n",
    "    substitution to SpecRamdonCrop \n",
    "    divide spectrogram into target length segments and get the average value\n",
    "    work only when target_len > spect_len\n",
    "    \"\"\"\n",
    "    def __init__(self, target_len=960, padding_method=\"circular\"):\n",
    "        self.target_len = target_len\n",
    "        self.padding_method = padding_method \n",
    "\n",
    "    def __call__(self,img):\n",
    "        cnt_len = img.shape[-1]\n",
    "\n",
    "        if cnt_len > self.target_len:\n",
    "            sub_imgs = [ img[..., idx:idx + self.target_len] for idx in range(0,cnt_len, self.target_len)]\n",
    "            sub_imgs = sub_imgs[:-1] # since the last slice is unintact, we discard it\n",
    "            sub_imgs = np.concat(sub_imgs)\n",
    "            img = sub_imgs.mean(dim=0, keepdim=True)\n",
    "\n",
    "        return img\n",
    "\n",
    "class SpecRepeat(object):\n",
    "    \"\"\"Repeat channel layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # if type(img) is tuple:\n",
    "        #     attn_mask = img[1]\n",
    "        #     img = img[0]\n",
    "        #     if img.shape[0] == 1:\n",
    "        #         img = img.repeat(3,1,1)\n",
    "            \n",
    "        #     if attn_mask is not None and attn_mask.shape[0] == 1:\n",
    "        #         attn_mask = attn_mask.repeat(3,1,1)\n",
    "        #         return img,attn_mask\n",
    "        #     elif attn_mask is None:\n",
    "        #         return img\n",
    "        if img.shape[0] == 1:\n",
    "            img = np.tile( img, (3,1,1))\n",
    "        return img\n",
    "\n",
    "def get_spectrogram(target_path):\n",
    "        try:\n",
    "            data = np.load(target_path)\n",
    "            if data.dtype == np.half:\n",
    "                data = data.astype(np.float32)\n",
    "        except FileNotFoundError:\n",
    "            print(f'load np logMel failed when loading {target_path}')\n",
    "            return None\n",
    "\n",
    "        assert data.shape[0] == 1 and data.ndim == 3, f\"Data {target_path} shape is {data.shape}, corrupted!\"\n",
    "\n",
    "        return data\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing mmcv: No module named 'mmcv'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: mmcv. Run `pip install mmcv`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUniMus/OpenJMLA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# sample rate: 16k\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/active/text2vital/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:553\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    552\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 553\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/active/text2vital/venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:540\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload\u001b[38;5;241m=\u001b[39mforce_download)\n",
      "File \u001b[0;32m~/projects/active/text2vital/venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:365\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[0;32m~/projects/active/text2vital/venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:197\u001b[0m, in \u001b[0;36mcheck_imports\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[0;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: mmcv. Run `pip install mmcv`"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = AutoModel.from_pretrained('UniMus/OpenJMLA', trust_remote_code=True)\n",
    "device = model.device\n",
    "# sample rate: 16k\n",
    "music_path = './1.mp3'\n",
    "# 1. get logmelspectrogram\n",
    "# get the file wav_to_mel.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "from wav_to_mel import wav_to_mel\n",
    "lms = wav_to_mel(music_path)\n",
    "\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "# get the file transforms.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "from transforms import Normalize, SpecRandomCrop, SpecPadding, SpecRepeat\n",
    "transforms = [ Normalize(-4.5, 4.5), SpecRandomCrop(target_len=2992), SpecPadding(target_len=2992), SpecRepeat() ]\n",
    "lms = lms.numpy()\n",
    "for trans in transforms:\n",
    "    lms = trans(lms)\n",
    "\n",
    "# 2. template of input\n",
    "input_dic = dict()\n",
    "input_dic['filenames'] = [music_path.split('/')[-1]]\n",
    "input_dic['ans_crds'] = [0]\n",
    "input_dic['audio_crds'] = [0]\n",
    "input_dic['attention_mask'] = torch.tensor([[1, 1, 1, 1, 1]]).to(device)\n",
    "input_dic['input_ids'] = torch.tensor([[1, 694, 5777, 683, 13]]).to(device)\n",
    "input_dic['spectrogram'] = torch.from_numpy(lms).unsqueez(dim=0).to(device)\n",
    "# 3. generation\n",
    "model.eval()\n",
    "gen_ids = model.forward_test(input)\n",
    "gen_text = model.neck.tokenizer.batch_decode(gen_ids.clip(0))\n",
    "# 4. Post-processing\n",
    "# Given that the training data may contain biases, the generated texts might need some straightforward post-processing to ensure accuracy.\n",
    "# In future versions, we will enhance the quality of the data.\n",
    "gen_text = gen_text.split('<s>')[-1].split('\\n')[0].strip()\n",
    "gen_text = gen_text.replace(' in Chinese','')\n",
    "gen_text = gen_text.replace(' Chinese','')\n",
    "print(gen_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
